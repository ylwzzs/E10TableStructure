#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…ç†è„šæœ¬
åŠŸèƒ½ï¼šæ¸…ç†JSONæ–‡ä»¶ä¸­çš„é‡å¤è¡¨æ•°æ®
"""

import json
import logging
from pathlib import Path
from datetime import datetime
import argparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('clean_data.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataCleaner:
    def __init__(self, json_file="all_tables.json"):
        self.json_file = Path(json_file)
        self.tables_data = []
        
    def load_data(self):
        """åŠ è½½JSONæ•°æ®"""
        if not self.json_file.exists():
            print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {self.json_file}")
            return False
            
        try:
            print(f"ğŸ“– åŠ è½½JSONæ•°æ®: {self.json_file}")
            with open(self.json_file, 'r', encoding='utf-8') as f:
                self.tables_data = json.load(f)
            
            print(f"ğŸ“Š åŠ è½½äº† {len(self.tables_data)} ä¸ªè¡¨çš„æ•°æ®")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def analyze_duplicates(self):
        """åˆ†æé‡å¤æ•°æ®"""
        if not self.tables_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return
        
        print("ğŸ” åˆ†æé‡å¤æ•°æ®...")
        
        # ç»Ÿè®¡è¡¨åå‡ºç°æ¬¡æ•°
        table_name_counts = {}
        for table in self.tables_data:
            table_name = table.get('table_name', '')
            if table_name:
                table_name_counts[table_name] = table_name_counts.get(table_name, 0) + 1
        
        # æ‰¾å‡ºé‡å¤çš„è¡¨å
        duplicates = {name: count for name, count in table_name_counts.items() if count > 1}
        
        if duplicates:
            print(f"âš ï¸  å‘ç° {len(duplicates)} ä¸ªé‡å¤çš„è¡¨å:")
            for table_name, count in sorted(duplicates.items()):
                print(f"   - {table_name}: {count} æ¬¡")
        else:
            print("âœ… æ²¡æœ‰å‘ç°é‡å¤çš„è¡¨å")
        
        return duplicates
    
    def clean_duplicates(self, strategy='latest'):
        """æ¸…ç†é‡å¤æ•°æ®"""
        if not self.tables_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯æ¸…ç†")
            return False
        
        print("ğŸ§¹ å¼€å§‹æ¸…ç†é‡å¤æ•°æ®...")
        original_count = len(self.tables_data)
        
        # ä½¿ç”¨å­—å…¸å»é‡
        unique_tables = {}
        duplicates = []
        
        for table in self.tables_data:
            table_name = table.get('table_name', '')
            if not table_name:
                # è·³è¿‡æ²¡æœ‰è¡¨åçš„è®°å½•
                duplicates.append(table)
                continue
                
            if table_name in unique_tables:
                # å‘ç°é‡å¤ï¼Œæ ¹æ®ç­–ç•¥é€‰æ‹©ä¿ç•™å“ªä¸ª
                existing = unique_tables[table_name]
                
                if strategy == 'latest':
                    # ä¿ç•™æœ€æ–°çš„
                    existing_updated = existing.get('last_updated', '')
                    current_updated = table.get('last_updated', '')
                    
                    if current_updated > existing_updated:
                        # å½“å‰è®°å½•æ›´æ–°ï¼Œæ›¿æ¢æ—§çš„
                        duplicates.append(existing)
                        unique_tables[table_name] = table
                    else:
                        # ä¿ç•™ç°æœ‰çš„ï¼Œå½“å‰è®°å½•æ˜¯é‡å¤çš„
                        duplicates.append(table)
                elif strategy == 'first':
                    # ä¿ç•™ç¬¬ä¸€ä¸ª
                    duplicates.append(table)
                elif strategy == 'merge':
                    # åˆå¹¶æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰éç©ºå­—æ®µï¼‰
                    merged = existing.copy()
                    for key, value in table.items():
                        if value and (key not in merged or not merged[key]):
                            merged[key] = value
                    unique_tables[table_name] = merged
                    duplicates.append(table)
            else:
                unique_tables[table_name] = table
        
        # æ›´æ–°æ•°æ®åˆ—è¡¨
        self.tables_data = list(unique_tables.values())
        
        cleaned_count = len(self.tables_data)
        duplicate_count = len(duplicates)
        
        print(f"âœ… æ¸…ç†å®Œæˆ:")
        print(f"   - åŸå§‹æ•°æ®: {original_count} ä¸ªè¡¨")
        print(f"   - æ¸…ç†å: {cleaned_count} ä¸ªè¡¨")
        print(f"   - åˆ é™¤é‡å¤: {duplicate_count} ä¸ª")
        print(f"   - å‡å°‘: {original_count - cleaned_count} ä¸ª")
        
        logger.info(f"æ¸…ç†é‡å¤æ•°æ®: åŸå§‹ {original_count} -> æ¸…ç†å {cleaned_count}, åˆ é™¤ {duplicate_count} ä¸ªé‡å¤")
        
        if duplicates:
            print(f"ğŸ—‘ï¸  åˆ é™¤çš„é‡å¤è¡¨ (å‰10ä¸ª):")
            for i, dup in enumerate(duplicates[:10]):
                print(f"   {i+1}. {dup.get('table_name', 'Unknown')} - {dup.get('file', 'Unknown file')}")
            if len(duplicates) > 10:
                print(f"   ... è¿˜æœ‰ {len(duplicates) - 10} ä¸ªé‡å¤è¡¨")
        
        return True
    
    def save_data(self):
        """ä¿å­˜æ¸…ç†åçš„æ•°æ®"""
        try:
            print("ğŸ’¾ ä¿å­˜æ¸…ç†åçš„æ•°æ®...")
            
            # æŒ‰è¡¨åæ’åº
            self.tables_data.sort(key=lambda x: x.get('table_name', ''))
            
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_file = self.json_file.with_suffix('.json.backup')
            if self.json_file.exists():
                import shutil
                shutil.copy2(self.json_file, backup_file)
                print(f"ğŸ“¦ å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
            
            # ä¿å­˜æ–°æ•°æ®
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(self.tables_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜: {len(self.tables_data)} ä¸ªè¡¨")
            logger.info(f"ä¿å­˜æ¸…ç†åçš„æ•°æ®: {len(self.tables_data)} ä¸ªè¡¨")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def validate_data(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        if not self.tables_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯éªŒè¯")
            return False
        
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        missing_fields = {}
        empty_table_names = 0
        
        for i, table in enumerate(self.tables_data):
            table_name = table.get('table_name', '')
            if not table_name:
                empty_table_names += 1
                continue
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['table_name', 'file']
            for field in required_fields:
                if field not in table or not table[field]:
                    if field not in missing_fields:
                        missing_fields[field] = []
                    missing_fields[field].append(table_name)
        
        print(f"ğŸ“Š æ•°æ®éªŒè¯ç»“æœ:")
        print(f"   - æ€»è¡¨æ•°: {len(self.tables_data)}")
        print(f"   - ç©ºè¡¨å: {empty_table_names}")
        
        if missing_fields:
            print(f"   - ç¼ºå¤±å­—æ®µ:")
            for field, tables in missing_fields.items():
                print(f"     * {field}: {len(tables)} ä¸ªè¡¨ç¼ºå¤±")
        else:
            print(f"   - æ‰€æœ‰å¿…éœ€å­—æ®µå®Œæ•´")
        
        return len(missing_fields) == 0 and empty_table_names == 0

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®æ¸…ç†è„šæœ¬')
    parser.add_argument('--json-file', default='all_tables.json', help='JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analyze', action='store_true', help='åªåˆ†æé‡å¤æ•°æ®ï¼Œä¸æ¸…ç†')
    parser.add_argument('--clean', action='store_true', help='æ¸…ç†é‡å¤æ•°æ®')
    parser.add_argument('--strategy', choices=['latest', 'first', 'merge'], default='latest', 
                       help='æ¸…ç†ç­–ç•¥: latest(ä¿ç•™æœ€æ–°), first(ä¿ç•™ç¬¬ä¸€ä¸ª), merge(åˆå¹¶æ•°æ®)')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    parser.add_argument('--backup', action='store_true', help='åˆ›å»ºå¤‡ä»½')
    
    args = parser.parse_args()
    
    cleaner = DataCleaner(args.json_file)
    
    # åŠ è½½æ•°æ®
    if not cleaner.load_data():
        exit(1)
    
    # åˆ†æé‡å¤æ•°æ®
    duplicates = cleaner.analyze_duplicates()
    
    # éªŒè¯æ•°æ®
    if args.validate:
        cleaner.validate_data()
    
    # æ¸…ç†æ•°æ®
    if args.clean or (not args.analyze and duplicates):
        if duplicates:
            print(f"\nğŸ”„ å‘ç°é‡å¤æ•°æ®ï¼Œå¼€å§‹æ¸…ç†...")
            if cleaner.clean_duplicates(args.strategy):
                cleaner.save_data()
                print("ğŸ‰ æ•°æ®æ¸…ç†å®Œæˆï¼")
            else:
                print("âŒ æ•°æ®æ¸…ç†å¤±è´¥")
                exit(1)
        else:
            print("âœ… æ²¡æœ‰é‡å¤æ•°æ®éœ€è¦æ¸…ç†")
    
    # æœ€ç»ˆéªŒè¯
    if args.clean or (not args.analyze and duplicates):
        print("\nğŸ” æœ€ç»ˆéªŒè¯...")
        cleaner.validate_data()

if __name__ == "__main__":
    main() 