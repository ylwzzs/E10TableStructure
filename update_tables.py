#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡¨ç»“æ„æ›´æ–°è„šæœ¬
åŠŸèƒ½ï¼š
1. å¢é‡æ›´æ–°HTMLæ–‡ä»¶
2. è½¬æ¢ä¸ºUTF-8ç¼–ç 
3. æ›´æ–°JSONæ–‡ä»¶
4. è‡ªåŠ¨æ¨é€åˆ°GitHub
"""

import os
import json
import hashlib
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
import chardet
from bs4 import BeautifulSoup
import argparse
import logging
from tqdm import tqdm
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('update_tables.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TableStructureUpdater:
    def __init__(self, source_dir=".", json_file="all_tables.json", backup_dir="backup"):
        self.source_dir = Path(source_dir)
        self.json_file = Path(json_file)
        self.backup_dir = Path(backup_dir)
        self.hash_file = Path("file_hashes.json")
        self.tables_data = []
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        self.backup_dir.mkdir(exist_ok=True)
        
    def detect_encoding(self, file_path):
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                result = chardet.detect(raw_data)
                return result['encoding']
        except Exception as e:
            logger.error(f"æ£€æµ‹ç¼–ç å¤±è´¥ {file_path}: {e}")
            return 'utf-8'
    
    def convert_to_utf8(self, file_path):
        """å°†æ–‡ä»¶è½¬æ¢ä¸ºUTF-8ç¼–ç """
        try:
            encoding = self.detect_encoding(file_path)
            if encoding and encoding.lower() != 'utf-8':
                logger.info(f"è½¬æ¢ç¼–ç  {file_path}: {encoding} -> UTF-8")
                
                # è¯»å–åŸæ–‡ä»¶
                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = self.backup_dir / f"{file_path.name}.{encoding}"
                shutil.copy2(file_path, backup_path)
                logger.info(f"å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_path}")
                
                # å†™å…¥UTF-8æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                return True
            return False
        except Exception as e:
            logger.error(f"è½¬æ¢ç¼–ç å¤±è´¥ {file_path}: {e}")
            return False
    
    def parse_html_file(self, file_path):
        """è§£æHTMLæ–‡ä»¶ï¼Œæå–è¡¨ç»“æ„ä¿¡æ¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # æå–è¡¨åï¼ˆä»æ–‡ä»¶åæˆ–é¡µé¢å†…å®¹ï¼‰
            table_name = file_path.stem
            if '(' in table_name and ')' in table_name:
                # å¤„ç†ç±»ä¼¼ "table_name(æè¿°)_id.html" çš„æ ¼å¼
                table_name = table_name.split('(')[0]
            
            # æå–ä¸­æ–‡æè¿°
            table_comment = ""
            description = ""
            
            # å°è¯•ä»é¡µé¢æ ‡é¢˜æˆ–å†…å®¹ä¸­æå–æè¿°
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.get_text().strip()
                if '(' in title_text and ')' in title_text:
                    table_comment = title_text.split('(')[1].split(')')[0]
                    description = title_text
            
            # å°è¯•ä»é¡µé¢å†…å®¹ä¸­æå–æ›´å¤šä¿¡æ¯
            body_text = soup.get_text()
            
            # æå–æ¨¡å—ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            module = ""
            if "æ¨¡å—" in body_text or "å¾®æœåŠ¡" in body_text:
                # ç®€å•çš„æ¨¡å—æå–é€»è¾‘ï¼Œå¯ä»¥æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´
                lines = body_text.split('\n')
                for line in lines:
                    if "æ¨¡å—" in line or "å¾®æœåŠ¡" in line:
                        module = line.strip()
                        break
            
            # æå–æ•°æ®åº“ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            db_name = ""
            if "æ•°æ®åº“" in body_text:
                lines = body_text.split('\n')
                for line in lines:
                    if "æ•°æ®åº“" in line:
                        db_name = line.strip()
                        break
            
            return {
                "table_name": table_name,
                "table_comment": table_comment,
                "module": module,
                "db_name": db_name,
                "description": description,
                "file": file_path.name,
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è§£æHTMLæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return None
    
    def load_file_hashes(self):
        """åŠ è½½æ–‡ä»¶å“ˆå¸Œè®°å½•"""
        if self.hash_file.exists():
            try:
                with open(self.hash_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½å“ˆå¸Œè®°å½•å¤±è´¥: {e}")
        return {}
    
    def save_file_hashes(self, hashes):
        """ä¿å­˜æ–‡ä»¶å“ˆå¸Œè®°å½•"""
        try:
            with open(self.hash_file, 'w', encoding='utf-8') as f:
                json.dump(hashes, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å“ˆå¸Œè®°å½•å¤±è´¥: {e}")
    
    def get_html_files(self):
        """è·å–æ‰€æœ‰HTMLæ–‡ä»¶"""
        return list(self.source_dir.glob("*.html"))
    
    def update_tables_data(self, force_update=False):
        """æ›´æ–°è¡¨ç»“æ„æ•°æ®"""
        html_files = self.get_html_files()
        file_hashes = self.load_file_hashes()
        updated_files = []
        converted_files = []
        skipped_files = []
        
        print(f"ğŸ“ å¼€å§‹å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶...")
        logger.info(f"å¼€å§‹å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒtqdmè¿›åº¦æ¡
        try:
            # å°è¯•ä½¿ç”¨tqdmè¿›åº¦æ¡
            with tqdm(total=len(html_files), desc="å¤„ç†æ–‡ä»¶", unit="ä¸ª", 
                     bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
                
                for i, html_file in enumerate(html_files):
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_description(f"å¤„ç† {html_file.name[:30]}...")
                    
                    current_hash = self.calculate_file_hash(html_file)
                    if current_hash is None:
                        pbar.update(1)
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    needs_update = force_update or html_file.name not in file_hashes or file_hashes[html_file.name] != current_hash
                    
                    if needs_update:
                        logger.info(f"å¤„ç†æ–‡ä»¶: {html_file.name}")
                        
                        # æ£€æŸ¥ç¼–ç ï¼Œå¦‚æœæ˜¯UTF-8ä¸”ä¸éœ€è¦å¼ºåˆ¶æ›´æ–°ï¼Œåˆ™è·³è¿‡
                        if not force_update:
                            encoding = self.detect_encoding(html_file)
                            if encoding and encoding.lower() == 'utf-8':
                                # åªæ›´æ–°å“ˆå¸Œè®°å½•ï¼Œä¸å¤„ç†æ–‡ä»¶
                                file_hashes[html_file.name] = current_hash
                                skipped_files.append(html_file.name)
                                pbar.update(1)
                                continue
                        
                        # è½¬æ¢ç¼–ç 
                        if self.convert_to_utf8(html_file):
                            converted_files.append(html_file.name)
                        
                        # è§£æHTML
                        table_data = self.parse_html_file(html_file)
                        if table_data:
                            # æ›´æ–°æˆ–æ·»åŠ è¡¨æ•°æ®
                            self.update_table_in_data(table_data)
                            updated_files.append(html_file.name)
                        
                        # æ›´æ–°å“ˆå¸Œè®°å½•
                        file_hashes[html_file.name] = current_hash
                    else:
                        # æ–‡ä»¶æ²¡æœ‰å˜åŒ–ï¼Œè·³è¿‡å¤„ç†
                        skipped_files.append(html_file.name)
                    
                    pbar.update(1)
                    
        except Exception as e:
            # å¦‚æœtqdmå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º
            print(f"âš ï¸  è¿›åº¦æ¡æ˜¾ç¤ºå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º: {e}")
            logger.warning(f"è¿›åº¦æ¡æ˜¾ç¤ºå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º: {e}")
            
            for i, html_file in enumerate(html_files):
                # æ¯å¤„ç†100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if i % 100 == 0 or i == len(html_files) - 1:
                    progress = (i + 1) / len(html_files) * 100
                    print(f"ğŸ“Š è¿›åº¦: {i+1}/{len(html_files)} ({progress:.1f}%) - å¤„ç†: {html_file.name[:50]}...")
                
                current_hash = self.calculate_file_hash(html_file)
                if current_hash is None:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                needs_update = force_update or html_file.name not in file_hashes or file_hashes[html_file.name] != current_hash
                
                if needs_update:
                    logger.info(f"å¤„ç†æ–‡ä»¶: {html_file.name}")
                    
                    # æ£€æŸ¥ç¼–ç ï¼Œå¦‚æœæ˜¯UTF-8ä¸”ä¸éœ€è¦å¼ºåˆ¶æ›´æ–°ï¼Œåˆ™è·³è¿‡
                    if not force_update:
                        encoding = self.detect_encoding(html_file)
                        if encoding and encoding.lower() == 'utf-8':
                            # åªæ›´æ–°å“ˆå¸Œè®°å½•ï¼Œä¸å¤„ç†æ–‡ä»¶
                            file_hashes[html_file.name] = current_hash
                            skipped_files.append(html_file.name)
                            continue
                    
                    # è½¬æ¢ç¼–ç 
                    if self.convert_to_utf8(html_file):
                        converted_files.append(html_file.name)
                    
                    # è§£æHTML
                    table_data = self.parse_html_file(html_file)
                    if table_data:
                        # æ›´æ–°æˆ–æ·»åŠ è¡¨æ•°æ®
                        self.update_table_in_data(table_data)
                        updated_files.append(html_file.name)
                    
                    # æ›´æ–°å“ˆå¸Œè®°å½•
                    file_hashes[html_file.name] = current_hash
                else:
                    # æ–‡ä»¶æ²¡æœ‰å˜åŒ–ï¼Œè·³è¿‡å¤„ç†
                    skipped_files.append(html_file.name)
        
        # ä¿å­˜å“ˆå¸Œè®°å½•
        self.save_file_hashes(file_hashes)
        
        print(f"âœ… æ›´æ–°å®Œæˆ: {len(updated_files)} ä¸ªæ–‡ä»¶æ›´æ–°, {len(converted_files)} ä¸ªæ–‡ä»¶è½¬æ¢ç¼–ç , {len(skipped_files)} ä¸ªæ–‡ä»¶è·³è¿‡")
        logger.info(f"æ›´æ–°å®Œæˆ: {len(updated_files)} ä¸ªæ–‡ä»¶æ›´æ–°, {len(converted_files)} ä¸ªæ–‡ä»¶è½¬æ¢ç¼–ç , {len(skipped_files)} ä¸ªæ–‡ä»¶è·³è¿‡")
        return updated_files, converted_files
    
    def clean_duplicate_data(self):
        """æ¸…ç†é‡å¤çš„è¡¨æ•°æ®"""
        if not self.tables_data:
            return
        
        print("ğŸ§¹ æ¸…ç†é‡å¤æ•°æ®...")
        original_count = len(self.tables_data)
        
        # ä½¿ç”¨å­—å…¸å»é‡ï¼Œä»¥table_nameä¸ºé”®
        unique_tables = {}
        duplicates = []
        
        for table in self.tables_data:
            table_name = table.get('table_name', '')
            if table_name in unique_tables:
                # å‘ç°é‡å¤ï¼Œä¿ç•™æœ€æ–°çš„
                existing = unique_tables[table_name]
                existing_updated = existing.get('last_updated', '')
                current_updated = table.get('last_updated', '')
                
                if current_updated > existing_updated:
                    # å½“å‰è®°å½•æ›´æ–°ï¼Œæ›¿æ¢æ—§çš„
                    duplicates.append(existing)
                    unique_tables[table_name] = table
                else:
                    # ä¿ç•™ç°æœ‰çš„ï¼Œå½“å‰è®°å½•æ˜¯é‡å¤çš„
                    duplicates.append(table)
            else:
                unique_tables[table_name] = table
        
        # æ›´æ–°æ•°æ®åˆ—è¡¨
        self.tables_data = list(unique_tables.values())
        
        cleaned_count = len(self.tables_data)
        duplicate_count = len(duplicates)
        
        print(f"âœ… æ¸…ç†å®Œæˆ: åŸå§‹ {original_count} ä¸ªè¡¨ -> æ¸…ç†å {cleaned_count} ä¸ªè¡¨ï¼Œåˆ é™¤ {duplicate_count} ä¸ªé‡å¤")
        logger.info(f"æ¸…ç†é‡å¤æ•°æ®: åŸå§‹ {original_count} ä¸ªè¡¨ -> æ¸…ç†å {cleaned_count} ä¸ªè¡¨ï¼Œåˆ é™¤ {duplicate_count} ä¸ªé‡å¤")
        
        if duplicates:
            logger.info(f"åˆ é™¤çš„é‡å¤è¡¨: {[d.get('table_name', '') for d in duplicates[:10]]}...")
    
    def update_table_in_data(self, new_table_data):
        """æ›´æ–°æˆ–æ·»åŠ è¡¨æ•°æ®åˆ°ç°æœ‰æ•°æ®ä¸­"""
        # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥è¡¨
        existing_index = None
        for i, table in enumerate(self.tables_data):
            if table.get('table_name') == new_table_data['table_name']:
                existing_index = i
                break
        
        if existing_index is not None:
            # æ›´æ–°ç°æœ‰è®°å½•
            self.tables_data[existing_index].update(new_table_data)
            logger.info(f"æ›´æ–°è¡¨: {new_table_data['table_name']}")
        else:
            # æ·»åŠ æ–°è®°å½•
            self.tables_data.append(new_table_data)
            logger.info(f"æ·»åŠ æ–°è¡¨: {new_table_data['table_name']}")
    
    def load_existing_data(self):
        """åŠ è½½ç°æœ‰çš„JSONæ•°æ®"""
        if self.json_file.exists():
            try:
                print("ğŸ“– åŠ è½½ç°æœ‰JSONæ•°æ®...")
                with open(self.json_file, 'r', encoding='utf-8') as f:
                    self.tables_data = json.load(f)
                print(f"ğŸ“Š å·²åŠ è½½ {len(self.tables_data)} ä¸ªè¡¨çš„æ•°æ®")
                logger.info(f"åŠ è½½ç°æœ‰æ•°æ®: {len(self.tables_data)} ä¸ªè¡¨")
                
                # æ¸…ç†é‡å¤æ•°æ®
                self.clean_duplicate_data()
                
            except Exception as e:
                logger.error(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
                self.tables_data = []
        else:
            self.tables_data = []
    
    def save_json_data(self):
        """ä¿å­˜JSONæ•°æ®"""
        try:
            print("ğŸ’¾ ä¿å­˜JSONæ•°æ®...")
            # æŒ‰è¡¨åæ’åº
            self.tables_data.sort(key=lambda x: x.get('table_name', ''))
            
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(self.tables_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ä¿å­˜å®Œæˆ: {len(self.tables_data)} ä¸ªè¡¨")
            logger.info(f"ä¿å­˜JSONæ•°æ®: {len(self.tables_data)} ä¸ªè¡¨")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ•°æ®å¤±è´¥: {e}")
            return False
    
    def git_operations(self, commit_message=None):
        """æ‰§è¡ŒGitæ“ä½œ"""
        try:
            print("ğŸ” æ£€æŸ¥GitçŠ¶æ€...")
            # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
            result = subprocess.run(['git', 'status', '--porcelain'], 
                                  capture_output=True, text=True, encoding='utf-8')
            
            if not result.stdout.strip():
                print("âœ… æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡Gitæ“ä½œ")
                logger.info("æ²¡æœ‰æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡Gitæ“ä½œ")
                return True
            
            print("ğŸ“ æ‰§è¡ŒGitæ“ä½œ...")
            # æ·»åŠ æ‰€æœ‰æ–‡ä»¶
            subprocess.run(['git', 'add', '.'], check=True)
            print("âœ… Git add å®Œæˆ")
            logger.info("Git add å®Œæˆ")
            
            # æäº¤
            if not commit_message:
                commit_message = f"è‡ªåŠ¨æ›´æ–°è¡¨ç»“æ„ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            
            subprocess.run(['git', 'commit', '-m', commit_message], check=True)
            print(f"âœ… Git commit å®Œæˆ: {commit_message}")
            logger.info(f"Git commit å®Œæˆ: {commit_message}")
            
            # æ¨é€åˆ°è¿œç¨‹ä»“åº“
            print("ğŸš€ æ¨é€åˆ°è¿œç¨‹ä»“åº“...")
            subprocess.run(['git', 'push'], check=True)
            print("âœ… Git push å®Œæˆ")
            logger.info("Git push å®Œæˆ")
            
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Gitæ“ä½œå¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"Gitæ“ä½œå¼‚å¸¸: {e}")
            return False
    
    def run_update(self, force_update=False, auto_git=True, commit_message=None):
        """è¿è¡Œå®Œæ•´çš„æ›´æ–°æµç¨‹"""
        print("ğŸ”„ å¼€å§‹è¡¨ç»“æ„æ›´æ–°æµç¨‹...")
        logger.info("å¼€å§‹è¡¨ç»“æ„æ›´æ–°æµç¨‹")
        
        # 1. åŠ è½½ç°æœ‰æ•°æ®
        self.load_existing_data()
        
        # 2. æ›´æ–°è¡¨æ•°æ®
        updated_files, converted_files = self.update_tables_data(force_update)
        
        if not updated_files and not converted_files:
            print("âœ… æ²¡æœ‰æ–‡ä»¶éœ€è¦æ›´æ–°")
            logger.info("æ²¡æœ‰æ–‡ä»¶éœ€è¦æ›´æ–°")
            return True
        
        # 3. ä¿å­˜JSONæ•°æ®
        if not self.save_json_data():
            return False
        
        # 4. Gitæ“ä½œ
        if auto_git:
            if not self.git_operations(commit_message):
                return False
        
        print("ğŸ‰ è¡¨ç»“æ„æ›´æ–°æµç¨‹å®Œæˆ")
        logger.info("è¡¨ç»“æ„æ›´æ–°æµç¨‹å®Œæˆ")
        return True

def main():
    parser = argparse.ArgumentParser(description='è¡¨ç»“æ„æ›´æ–°è„šæœ¬')
    parser.add_argument('--source-dir', default='.', help='HTMLæ–‡ä»¶æºç›®å½•')
    parser.add_argument('--json-file', default='all_tables.json', help='è¾“å‡ºJSONæ–‡ä»¶å')
    parser.add_argument('--backup-dir', default='backup', help='å¤‡ä»½ç›®å½•')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰æ–‡ä»¶')
    parser.add_argument('--no-git', action='store_true', help='ä¸æ‰§è¡ŒGitæ“ä½œ')
    parser.add_argument('--commit-message', help='Gitæäº¤ä¿¡æ¯')
    
    args = parser.parse_args()
    
    updater = TableStructureUpdater(
        source_dir=args.source_dir,
        json_file=args.json_file,
        backup_dir=args.backup_dir
    )
    
    success = updater.run_update(
        force_update=args.force,
        auto_git=not args.no_git,
        commit_message=args.commit_message
    )
    
    if success:
        logger.info("æ›´æ–°æˆåŠŸå®Œæˆ")
        exit(0)
    else:
        logger.error("æ›´æ–°å¤±è´¥")
        exit(1)

if __name__ == "__main__":
    main() 